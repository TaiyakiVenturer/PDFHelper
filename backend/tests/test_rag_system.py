import os
import sys
from pathlib import Path

# ç¢ºä¿æ¸¬è©¦ç’°å¢ƒä½¿ç”¨ UTF-8 ç·¨ç¢¼ï¼ˆèˆ‡ Electron ç’°å¢ƒä¸€è‡´ï¼‰
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')

def find_project_root(max_attempts: int = 5) -> Path:
    current_dir = Path(__file__).resolve().parent
    attempts = 0
    while attempts < max_attempts:
        backend_path = current_dir / 'backend'
        frontend_path = current_dir / 'frontend'
        if backend_path.is_dir() and frontend_path.is_dir():
            return current_dir
        if current_dir.parent == current_dir:
            break
        current_dir = current_dir.parent
        attempts += 1
    raise FileNotFoundError("æ‰¾ä¸åˆ°åŒ…å« 'backend' å’Œ 'frontend' ç›®éŒ„çš„å°ˆæ¡ˆæ ¹ç›®éŒ„")

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
instance_path = os.path.join(str(project_root), "backend", "instance")

import argparse

from backend.services.rag_service import DocumentProcessor, EmbeddingService, ChromaVectorStore, RAGEngine
import backend.services.llm_service as llm_services

def main():
    parser = argparse.ArgumentParser(description="RAG ç³»çµ±åŸºç¤ŽåŠŸèƒ½æ¸¬è©¦")
    parser.add_argument("--json", type=str, default="", help="æ¸¬è©¦ç”¨ JSON æª”æ¡ˆåç¨±")
    parser.add_argument("--question", type=str, default="é€™å€‹æ–‡ä»¶çš„ä¸»é¡Œ?", help="æ¸¬è©¦æœå°‹çš„å•é¡Œ")
    
    parser.add_argument("--model", type=str, default="", help="LLM æ¨¡åž‹åç¨± (ä¾‹å¦‚: llama2, gemini-pro, gpt-4-turbo)")
    parser.add_argument("--apikey", type=str, default="", help="LLM API é‡‘é‘°")
    args = parser.parse_args()

    print("1. åˆå§‹åŒ– RAG å¼•æ“Ž")
    try:
        # åˆå§‹åŒ– DocumentProcessor
        document_processor = DocumentProcessor(
            instance_path=instance_path,
            min_chunk_size=100,
            max_chunk_size=1200,
            merge_short_chunks=True,
            verbose=True
        )
        # åˆå§‹åŒ– LLM Service (Gemini)
        llm_service = llm_services.GoogleService(
            model_name=args.model,
            api_key=args.apikey,
            verbose=True
        )
        # llm_service = llm_services.OllamaService(
        #     model_name=args.model,  # å¯ä¾å¯¦éš› API Key èˆ‡æ¨¡åž‹èª¿æ•´
        #     api_key=args.apikey,
        #     verbose=True
        # )

        # åˆå§‹åŒ– EmbeddingService
        embedding_service = EmbeddingService(
            llm_service_obj=llm_service,
            max_retries=3,
            retry_delay=1,
            verbose=True
        )
        # åˆå§‹åŒ– ChromaVectorStore
        vector_store = ChromaVectorStore(
            instance_path=instance_path,
            persist_directory_name="chroma_db",
            collection_cache_size=3,
            verbose=True
        )
        # åˆå§‹åŒ– RAGEngine
        rag = RAGEngine(
            document_processor_obj=document_processor,
            embedding_service_obj=embedding_service,
            chromadb_obj=vector_store,
            llm_service_obj=llm_service,
            verbose=True
        )
        print("âœ… RAG å¼•æ“Žåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ RAG å¼•æ“Žåˆå§‹åŒ–å¤±æ•—: {e}")
        return

    print("2. æ¸¬è©¦ Embedding ç”¢ç”Ÿ")
    try:
        test_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦å­—ä¸²ï¼Œç”¨æ–¼é©—è­‰embeddingåŠŸèƒ½ã€‚"
        embedding = rag.embedding_service.get_embedding(test_text)
        if embedding:
            print(f"âœ… Embedding ç”¢ç”ŸæˆåŠŸï¼Œç¶­åº¦: {len(embedding)}")
            print(f"   å‰5å€‹å€¼: {embedding[:5]}")
        else:
            print("âŒ Embedding ç”¢ç”Ÿå¤±æ•—")
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ Embedding æ™‚å‡ºéŒ¯: {e}")

    print("3. æ¸¬è©¦å‘é‡è³‡æ–™åº«ç´¢å¼•èˆ‡æœå°‹")
    try:
        success = rag.store_document_into_vectordb(args.json)
        if success:
            print("âœ… æ–‡ä»¶ç´¢å¼•æˆåŠŸ")
            search_results = rag.search(args.question, args.json, top_k=3)
            print(f"æœå°‹çµæžœæ•¸é‡: {len(search_results)}")
            for i, result in enumerate(search_results[:2]):
                print(f"çµæžœ {i+1}: {getattr(result, 'content', str(result))[:100]}...")
        else:
            print("âŒ æ–‡ä»¶ç´¢å¼•å¤±æ•—")
    except Exception as e:
        print(f"âŒ ç´¢å¼•æˆ–æœå°‹æ™‚å‡ºéŒ¯: {e}")

    print("4. æ¸¬è©¦ LLM å•ç­”åŠŸèƒ½ (å¯é¸)")
    try:
        collection_names = Path(args.json).stem
        response = rag.ask(args.question, collection_name=collection_names, top_k=3, include_sources=True)
        print(f"âœ… å•ç­”æ¸¬è©¦å®Œæˆ")
        print(f"ç‹€æ…‹: {getattr(response, 'status', None)}")
        print(f"æŸ¥è©¢: {getattr(response, 'query', None)}")
        print(f"å›žç­”: ", end="")
        for chunk in getattr(response, 'answer', []):
            print(getattr(chunk, 'text', str(chunk)), end="", flush=True)
        print()  # æ›è¡Œ
        print(f"éŸ¿æ‡‰æ™‚é–“: {getattr(response, 'response_time', 0):.2f}ç§’")
    except Exception as e:
        print(f"âš ï¸ å•ç­”æ¸¬è©¦å¤±æ•—: {e}")

    print("5. ç³»çµ±ä¿¡æ¯ç¸½çµ")
    try:
        system_info = rag.get_system_info(args.json)
        print("âœ… ç³»çµ±ä¿¡æ¯:")
        for key, value in system_info.items():
            print(f"   {key}: {value}")
    except Exception as e:
        print(f"âŒ ç²å–ç³»çµ±ä¿¡æ¯å¤±æ•—: {e}")

    print("\n" + "=" * 50)
    print("ðŸŽ‰ RAGç³»çµ±åŸºç¤ŽåŠŸèƒ½æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    main()
