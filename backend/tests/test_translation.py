import os
import sys
from pathlib import Path

def find_project_root(max_attempts: int = 5) -> Path:
    current_dir = Path(__file__).resolve().parent
    attempts = 0
    while attempts < max_attempts:
        backend_path = current_dir / 'backend'
        frontend_path = current_dir / 'frontend'
        if backend_path.is_dir() and frontend_path.is_dir():
            return current_dir
        if current_dir.parent == current_dir:
            break
        current_dir = current_dir.parent
        attempts += 1
    raise FileNotFoundError("æ‰¾ä¸åˆ°åŒ…å« 'backend' å’Œ 'frontend' ç›®éŒ„çš„å°ˆæ¡ˆæ ¹ç›®éŒ„")

project_root = find_project_root()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
instance_path = os.path.join(str(project_root), "backend", "instance")

import argparse
import time
import json

from backend.services.translation_service import Translator
import backend.services.llm_service as llm_services

def test_single_sentence(translator):
    print("ğŸ“ æ¸¬è©¦æ¨¡å¼ 1: å–®å¥ç¿»è­¯")
    test_cases = [
        {"text": "Autonomous Network Optimization and Dynamic Channel Allocation for Cognitive Radio-Based Consumer IoT", "type": "title"},
        {"text": "The heterogeneous environment of next-generation Consumer Internet of Things (CIoT) demands efficient resource utilization and reliable network services.", "type": "abstract"},
        {"text": "Machine learning algorithms enable autonomous decision-making in cognitive radio networks.", "type": "body"},
        {"text": "Zhang, L., & Wang, H. (2023). Cognitive radio networks: A comprehensive survey. IEEE Communications Surveys & Tutorials, 25(2), 1123-1145.", "type": "reference"}
    ]
    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹ {i} ({case['type']}):")
        print(f"åŸæ–‡: {case['text']}")
        start_time = time.time()
        translated = translator.translate_single_text(
            text=case['text'],
            content_type=case['type']
        )
        end_time = time.time()
        print(f"è­¯æ–‡: {translated}")
        print(f"â±ï¸  è€—æ™‚: {end_time - start_time:.2f}ç§’")
        print("-" * 30)
    if hasattr(translator, 'term_dictionary') and translator.term_dictionary:
        print(f"\nğŸ“š å»ºç«‹çš„è¡“èªå°ç…§è¡¨:")
        for en, zh in translator.term_dictionary.items():
            print(f"  {en} â†’ {zh}")

def test_partial_content(translator):
    print("ğŸ“„ æ¸¬è©¦æ¨¡å¼ 2: éƒ¨åˆ†å…§å®¹ç¿»è­¯")
    print("=" * 50)
    test_content = [
        {"type": "text", "text": "1. Introduction", "page_idx": 0, "text_level": 1},
        {"type": "text", "text": "Cognitive radio (CR) technology represents a paradigm shift in wireless communications.", "page_idx": 0, "text_level": 2},
        {"type": "text", "text": "The Internet of Things (IoT) ecosystem requires dynamic spectrum management for optimal performance.", "page_idx": 0, "text_level": 2}
    ]
    print(f"ğŸ“Š æ¸¬è©¦å…§å®¹åŒ…å« {len(test_content)} å€‹æ®µè½")
    for i, item in enumerate(test_content):
        if item.get('type') == 'text' and item.get('text'):
            print(f"\nğŸ“ æ®µè½ {i+1}:")
            print(f"åŸæ–‡: {item['text']}")
            content_type = "title" if item.get('text_level') == 1 else "body"
            context = test_content[i-1]['text'] if i > 0 else ""
            translated = translator.translate_single_text(
                text=item['text'],
                content_type=content_type,
                context=context
            )
            print(f"è­¯æ–‡: {translated}")
            print(f"é¡å‹: {content_type}")
            print("-" * 30)

def test_full_document(translator):
    print("ğŸ“š æ¸¬è©¦æ¨¡å¼ 3: å®Œæ•´æ–‡ä»¶ç¿»è­¯")
    print("=" * 50)
    content_list_path = None
    test_file_name = "SiLUæœŸæœ«å ±å‘Š"
    search_paths = [
        f"instance/mineru_outputs/{test_file_name}/auto/*_content_list.json"
    ]
    for pattern in search_paths:
        files = list(Path(".").glob(pattern))
        if files:
            content_list_path = files[0]
            break
    if not content_list_path:
        print("âŒ æ‰¾ä¸åˆ°content_list.jsonæ¸¬è©¦æª”æ¡ˆ")
        print("ğŸ’¡ è«‹ç¢ºèªä»¥ä¸‹ä½ç½®æ˜¯å¦æœ‰æª”æ¡ˆ:")
        for pattern in search_paths:
            print(f"   {pattern}")
        return
    print(f"ğŸ“ ä½¿ç”¨æ¸¬è©¦æª”æ¡ˆ: {content_list_path}")
    with open(content_list_path, 'r', encoding='utf-8') as f:
        content_list = json.load(f)
    text_items = [item for item in content_list if item.get('type') == 'text' and item.get('text')]
    total_chars = sum(len(item['text']) for item in text_items)
    print(f"ğŸ“Š æ–‡ä»¶çµ±è¨ˆ:")
    print(f"   ç¸½æ®µè½æ•¸: {len(text_items)}")
    print(f"   ç¸½å­—ç¬¦æ•¸: {total_chars}")
    print(f"   é ä¼°æ™‚é–“: {len(text_items) * 3}ç§’")
    start_time = time.time()
    output_path = translator.translate_content_list(
        content_list_path=str(content_list_path),
        buffer_time=1.8
    )
    end_time = time.time()
    print(f"\nâœ… ç¿»è­¯å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {output_path}")
    print(f"â±ï¸  ç¸½è€—æ™‚: {end_time - start_time:.1f}ç§’")
    print(f"ğŸ“š è¡“èªå°ç…§è¡¨: {len(translator.term_dictionary)} å€‹è¡“èª")

def main():
    parser = argparse.ArgumentParser(description="ç¿»è­¯æœå‹™æ¸¬è©¦å·¥å…·")
    parser.add_argument("--mode", type=str, choices=["single", "partial", "full"], default="single", help="æ¸¬è©¦æ¨¡å¼: single/partial/full")
    parser.add_argument("--provider", type=str, default="", help="LLM æœå‹™æä¾›è€… (ollama, google, openai)")
    parser.add_argument("--model", type=str, default="", help="LLM æ¨¡å‹åç¨± (ä¾‹å¦‚: llama2, gemini-pro, gpt-4-turbo)")
    parser.add_argument("--apikey", type=str, default="", help="LLM API é‡‘é‘°")
    args = parser.parse_args()

    # åˆå§‹åŒ– LLM Service (Gemini)
    llm_service = llm_services.GoogleService(
        model_name=args.model,
        api_key=args.apikey,
        verbose=True
    )
    # llm_service = llm_services.OllamaService(
    #     model_name=args.model,  # å¯ä¾å¯¦éš› API Key èˆ‡æ¨¡å‹èª¿æ•´
    #     api_key=args.apikey,
    #     verbose=True
    # )
    translator = Translator(instance_path=instance_path, llm_service_obj=llm_service, verbose=args.verbose)

    if args.mode == "single":
        test_single_sentence(translator)
    elif args.mode == "partial":
        test_partial_content(translator)
    elif args.mode == "full":
        test_full_document(translator)

if __name__ == "__main__":
    main()
